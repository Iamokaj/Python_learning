{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Today we are going to learn a new algorithm named as Decision Tree.\n",
    "with this we will dine with all of our classification algorithms and we apply them on a huge dataset.\n",
    "After this -- NLP, unsupervised Learning -- Kmeans , PCA\n",
    "Hyperparameter- Tuning of Fine-Tuning\n",
    "a. k-fold cross validation\n",
    "2. Grid Search\n",
    "3. Ensemble Techniques\n",
    "4. Train-test-validation test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decision Trees: \n",
    "Decision : Decision making is what the algorithm does while classifying the \n",
    "           observations.\n",
    "Tree : Non-linear data structure.\n",
    "Algorithm : series of steps to perform a task.\n",
    "    \n",
    "    \n",
    "The Decision tree algorithm belongs to the deterministic family of classifiers.\n",
    "and it is a model based technique.\n",
    "The Decision tree algorithm creates a tree like structure from the training dataset\n",
    "and then tree is traversed to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so, pehle hum ye seekehege ki agr decision tree bn gya to classification \n",
    "kitna aasan ho jaega classification krna or fir ye seekhege ki \n",
    "Decision Tree bnega kaise?\n",
    "we have just learned that if the tree is created then it is extremely easy for the model to \n",
    "generate predictions for the new instances.\n",
    "\n",
    "But, However few questions remained unanswered:\n",
    "1. How does the DT algo decides which attribute and threshold to choose for a paricular\n",
    "  node?\n",
    "    Mathematically , it can be written as--- (K,t_k)\n",
    "    k = attribute name\n",
    "    t_k = Threshold value.\n",
    "\n",
    "2. How does the DT algorithm decides what should be the optimal value for the depth \n",
    "of the tree.\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The Decision tree implementation of sklearn will continue creating the tree\n",
    "with more and more nodes unless and until all instances are correctly classified\n",
    "\n",
    "Humara Ped(tree) tb tak badhega yea bnta chla jayega jabtak sabhi instances \n",
    "shi yea theek classify na ho jae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This phenomenon is known as overfitting in Machine Learning.\n",
    "The advantage of DT algorithm is that it can deduce numerous relationships\n",
    "in the training data and attempts to always get an accuracy of around 100%\n",
    "on training dataset, but it can also be thought of as its biggest disadvantaf=ge\n",
    "\"Jab bhi aap DT ka use karege --- is baat humesha khayal rakhna  ki :  \"\n",
    "\n",
    "     \"Always remember to fine tune the max_depth parameter to get the optimal value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Do you think any another disadvantage of big tree instead of overfitting?\n",
    "keep in mind, that the larger the tree is, the more time it will\n",
    "take for traversal to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The Decision Tree is the most powerful ML algorithms existed.\n",
    "Despite all its flaws, it can work well with moderately large \n",
    "and complex datasets with \"n\" different classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris,load_digits,load_breast_cancer,load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "digits = load_digits()\n",
    "cancer = load_breast_cancer()\n",
    "wine = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtf = DecisionTreeClassifier(max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9553571428571429"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.92991497, 0.07008503])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_depth  badhane s accuracy humesha increase hoti h , training data ki.\n",
    "Now applying on load_digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtf = DecisionTreeClassifier(max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9873793615441723"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.00242056, 0.01015932, 0.00736232,\n",
       "       0.01963022, 0.        , 0.        , 0.        , 0.00160656,\n",
       "       0.03472582, 0.00459855, 0.0302727 , 0.01466939, 0.00151392,\n",
       "       0.00165024, 0.        , 0.        , 0.01361501, 0.02214034,\n",
       "       0.0024109 , 0.11145438, 0.00196248, 0.        , 0.        ,\n",
       "       0.00963516, 0.07077577, 0.05658336, 0.01057937, 0.01326752,\n",
       "       0.00288365, 0.        , 0.        , 0.00693535, 0.05910466,\n",
       "       0.        , 0.0798156 , 0.00369095, 0.05791741, 0.        ,\n",
       "       0.        , 0.        , 0.07836929, 0.05463948, 0.00655518,\n",
       "       0.00168213, 0.00810554, 0.        , 0.        , 0.        ,\n",
       "       0.0073453 , 0.        , 0.01450276, 0.00166323, 0.065002  ,\n",
       "       0.        , 0.        , 0.        , 0.00580929, 0.02009911,\n",
       "       0.0627816 , 0.00636111, 0.00432072, 0.01138171])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fine - Tuning : Hyperparameter Tuning helps in achieving better accuracy on training and testing dataset.\n",
    "Usually it helps us to remove overfitting.\n",
    "1. Grid Search\n",
    "2. Ensemble Techniques\n",
    "3. K-fold cross validation\n",
    "4. validation set and test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. k- fold cross validation : isko detail m graphically hum pd chuke h lekin ek quick revision\n",
    "    In this algorithm will run K- times. Each time choosing a different subset of training\n",
    "    and testing. if k = 3 and dataset is divided into a,b,c three parts\n",
    "    \n",
    "    1st trial --- {b,c} --- Training set\n",
    "                  {a} ---- Test set\n",
    "        \n",
    "    2nd Trial --- {a,c} -- Training set\n",
    "                   (b) --- Test set\n",
    "It provides 3 scores.\n",
    "\n",
    "\n",
    "2. Grid-Search -- grid ka mtlb koi aisi cheez jiski 2 dimensions ho.\n",
    " It seraches for optimal value for a hyperparameter technique. It creates 2-D\n",
    "    grid of parameters along with the values you want to experiment with.\n",
    "    The grid search then computes all combination of these parameters.\n",
    "    \n",
    "    \n",
    "3. Ensemble Technique : Ensemble : ekathaa, ek-sath\n",
    "                         Technique : method or tarika\n",
    "Ensemble simply means a group.\n",
    "On principles, we have seen that some datasets are so convoluted or large or dirty that no individual\n",
    "ML algorithm can converge to the solution. So for such special datasets we combine two or more\n",
    "algorithms to form a group(ensemble). The ensemble can be applied on the dataset in a hope to find better solution.\n",
    "There are two common ensemble techniques in Ml:\n",
    "    1. Bagging (Bootstrap aggregation)\n",
    "    2. Boosting\n",
    "    3. stacking (we can use different different algorithm)\n",
    "Bagging refers to random sampling with replacement.\n",
    "It is a statistical technique consists in generating samples of size(B) called bootstrap samples from\n",
    "an initial dataset of size \"N\" by randomly drawing with replacements B observations.\n",
    "Only single type of algo is used.\n",
    "Eg: RandomForest.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
